[System]
# Centralized Training with Decentralized Execution (CTDE). Critic will be provided with all the global observations, while the actor will be
# provided only with the local observations:
CT_DE = false
# Select either a 'dicrete' or a 'continuous' action space: 
action_space = discrete
# Actions delay can be taken into account (i.e., action_delay=False) or not (i.e., action_delay=True). Remember that if you want to take into account this actions delay, then you must enable it in 'scenario_parameters.ini' file:
action_delay = false
# Number of old performed actions to take into account in the observation space during the learning process (it will be ignored if 'action_delay' is set to 'false'):
actions_size = 3
# Observations delay can be taken into account during learning (i.e., observation_delay=False) or not (i.e., observation_delay=True). Remember that if you want to take into account this observations delay, then you must enable it in 'scenario_parameters.ini' file:
observation_delay = false
# Define the observation space. Select observations among the following ones:
# "source distance", "source bearing", "source drift", "source time", "battery", "agents distances", "single-agent coverage area", "AVG agent-all distance",
# "AVG agent-not_spotting_agents distance", "AVG spotting_agents-source distance", "current N spotting agents", "current agent spotted the source".
observations = [
		"current agent spotted the source"
		]
# Define the action space. Select the actions among the following ones: ["distance", "angle"] 
actions = [
	   "distance",
	   "angle"
	   ]

[PPO Hyperparameters]
# Learning rate:
lr = 0.0003
# Discount factor:
gamma = 0.9
# Generalized Advantage Estimation (GAE) Parameter (->lambda):
gae_lambda = 0.85
# Clipping range:
policy_clip = 0.2
# Value function coefficient (the critic network is used as value function) to compute the Loss:
c_value = 0.5
# Entropy coefficient (if 0, then obviously entropy is not used when computing the Loss):
c_entropy = 0.01
# Actor first fully connected layer dimension:
A_fc1_dims = 256
# Actor second fully connected layer dimension:
A_fc2_dims = 256
# Critic first fully connected layer dimension:
C_fc1_dims = 256
# Critic second fully connected layer dimension:
C_fc2_dims = 256

[SAC Hyperparameters]
# Actor learning rate:
lr1 = 0.0003
# Critic and Value learning rate:
lr2 = 0.0003
# Entropy regularization coefficient (can be set either manually or to 'auto' or to 'None': the latter case will use the explicit assignment for the reward scale; it cannot be set manually if also the reward scale is set):
alpha = auto
# Discount factor:
gamma = 0.99
# Magnitude 'tau' of the target Q update during the SAC 'soft model update': 
tau = 0.005
# It can be set either manually or equal to 'None' (it cannot be set manually if also 'alpha' is set manually):
reward_scale = 2
# Actor first fully connected layer dimension:
A_fc1_dims = 256
# Actor second fully connected layer dimension:
A_fc2_dims = 256
# Critic first fully connected layer dimension:
C_fc1_dims = 256
# Critic second fully connected layer dimension:
C_fc2_dims = 256
# Value first fully connected layer dimension:
V_fc1_dims = 256
# Value second fully connected layer dimension:
V_fc2_dims = 256

[General Hyperparameters]
# Batch size:
batch_size = 5
# Sigma value for action selection (if a continuous action space is used):
action_sigma = 0.6
# Decay rate for 'action_sigma':
action_sigma_decay_rate = 0.05 
# Minimum value for 'action_sigma':
action_sigma_min = 0.1
# Number of epochs:
n_episodes = 500
# Set how often to learn (it can be the learning clock and it could be set equal to the system clock):
N = 15

[Terminal Conditions]
# Percentage of UAVs which are supposed to spot the audio source at the same time:
perc_uavs_detecting_the_source = 0.3
# Maximum percentage of UAVs whose battery can be considered discharged: 
perc_uavs_discharged = 0.60
# Enable/Disable the following conditions to create some terminal conditions for each episode. If none of them is enabled, then the task (and the episode accordingly)
# will be persistent (i.e., 'running forever'):
task_ending = false
task_success = true
time_failure = true
battery_failure = false

[Reward]
# The reward can be cumulative (i.e., looking at the last 'N' rewards, when set to True) or not (when set to False):
cumulative_reward = true
# The step size allowing for the cumulative reward BACKupdating (if set equal to the memory size (i.e., batch_size), then the BACKupdate on the cumulative reward will be obviously performed only on the most recent time instat):
cumulative_rew_size = 3
# Agent reward weight (associated with all those rewards related to the local observation of individual agents, e.g., battery, position, ...):
W_agent = 1
# Agent system weight (associated with all those rewards related to the global observation, e.g., last source distance, time, ...):
W_system = 1
# Agent-source distance weight:
Wa = 1
# Agent-source bearing weight:
Wb = 1
# Agent-source drift weight:
Wc = 1
# Agent-source time weight:
Wd = 1
# Battery agent weight:
We = 1
# Agent-all distances weight:
Wf = 1
# Area coverage weight:
Wg = 1
# AVG agent-all distance weight:
Wh = 1
# AVG agent-not_spotting_agents distance weight:
Wi = 1
# AVG agent-spotting_agents distance weight:
Wl = 1
# Current N spotting agents weight (where N=N_UAVS*perc_uavs_detecting_the_source):
Wm = 1
# Current agent source spotting weight:
Wn = 1
